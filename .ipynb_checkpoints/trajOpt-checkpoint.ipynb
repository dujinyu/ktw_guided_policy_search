{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.linalg\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "def print_np(x):\n",
    "    print (\"Type is %s\" % (type(x)))\n",
    "    print (\"Shape is %s\" % (x.shape,))\n",
    "    print (\"Values are: \\n%s\" % (x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import model\n",
    "import cost\n",
    "import iLQR\n",
    "import poliOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class trajOpt:\n",
    "    def __init__(self,name,horizon):\n",
    "        # class name\n",
    "        self.name = name\n",
    "        \n",
    "        # given policy\n",
    "        # self.policy = policy\n",
    "        \n",
    "        # model & cost\n",
    "        self.model = model.unicycle('paul')\n",
    "        self.cost = cost.unicycle('john')\n",
    "           \n",
    "        # input & output size\n",
    "        self.ix = self.model.ix\n",
    "        self.iu = self.model.iu\n",
    "\n",
    "        # iLQR parameters\n",
    "        self.verbosity = True\n",
    "        self.dlamda = 1.0\n",
    "        self.lamda = 1.0\n",
    "        self.lamdaFactor = 1.6\n",
    "        self.lamdaMax = 1e10\n",
    "        self.lamdaMin = 1e-6\n",
    "        self.tolFun = 1e-2\n",
    "        self.tolGrad = 1e-4\n",
    "        self.maxIter = 5000\n",
    "        self.zMin = 0\n",
    "        self.last_head = True\n",
    "        self.dV = np.zeros((1,2))\n",
    "        self.N = horizon\n",
    "        \n",
    "        # dual variable\n",
    "        self.eta = 1\n",
    "        \n",
    "        # constraint variable\n",
    "        self.epsilon = 100\n",
    "        \n",
    "        # initial trajectory\n",
    "        self.x0 = np.zeros(self.model.ix)\n",
    "        self.x0cov = np.identity(self.model.ix)\n",
    "        self.x = np.zeros((self.N+1,self.model.ix))\n",
    "        self.S = np.tile(0.01*np.identity(self.model.ix),[self.N+1,1,1])\n",
    "        self.u = np.zeros((self.N,self.model.iu))\n",
    "#         self.A = np.tile(np.identity(self.model.iu),[self.N,1,1])\n",
    "        self.C = np.tile(0.01*np.identity(self.model.ix+  self.model.iu),[self.N+1,1,1])\n",
    "        \n",
    "        # next trajectroy\n",
    "        self.xnew = np.zeros((self.N+1,self.model.ix))\n",
    "        self.unew = np.zeros((self.N,self.model.iu))\n",
    "        self.Snew = np.tile(0.01*np.identity(self.model.ix),[self.N+1,1,1])\n",
    "        self.Cnew = np.tile(0.01*np.identity(self.model.ix + self.model.iu),[self.N+1,1,1])  \n",
    "        \n",
    "        # line-search step size\n",
    "        self.Alpha = np.power(10,np.linspace(0,-3,4))\n",
    "        \n",
    "        # feedforward, feedback gain\n",
    "        self.l = np.zeros((self.N,self.model.iu))\n",
    "        self.L = np.zeros((self.N,self.model.iu,self.model.ix))\n",
    "        \n",
    "        # input variance\n",
    "        self.Quu_save = np.zeros((self.N,self.model.iu,self.model.iu))\n",
    "        self.Quu_inv_save = np.zeros((self.N,self.model.iu,self.model.iu))\n",
    "        \n",
    "        # model jacobian\n",
    "        self.fx = np.zeros((self.N,self.model.ix,self.model.ix))\n",
    "        self.fu = np.zeros((self.N,self.model.ix,self.model.iu))\n",
    "        \n",
    "        # cost derivative\n",
    "        self.c = np.zeros(self.N+1)\n",
    "        self.cnew = np.zeros(self.N+1)\n",
    "        self.cx = np.zeros((self.N+1,self.model.ix))\n",
    "        self.cu = np.zeros((self.N,self.model.iu))\n",
    "        self.cxx = np.zeros((self.N+1,self.model.ix,self.model.ix))\n",
    "        self.cxu = np.zeros((self.N,self.model.ix,self.model.iu))\n",
    "        self.cuu = np.zeros((self.N,self.model.iu,self.model.iu))\n",
    "        \n",
    "        # value function\n",
    "        self.Vx = np.zeros((self.N+1,self.model.ix))\n",
    "        self.Vxx = np.zeros((self.N+1,self.model.ix,self.model.ix))\n",
    "        \n",
    "    def setEnv(self,policy,eta,epsilon) :\n",
    "    \n",
    "        self.policy = policy\n",
    "        self.eta = eta\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def getCost(self,x,u) :\n",
    "        \n",
    "        u_temp = np.vstack((u,np.zeros((1,self.model.iu))))\n",
    "        temp_c = self.cost.estimateCost(x,u_temp)\n",
    "        \n",
    "        return np.sum( temp_c )\n",
    "        \n",
    "    def getKL(self,x,u) :\n",
    "        \n",
    "        temp_KL = np.zeros(self.N)\n",
    "        mean, var = self.policy.getPolicy(x)\n",
    "        var_inv = np.linalg.inv(var)\n",
    "        for i in range(self.N) :\n",
    "            u_diff = mean[i,:] - u[i,:]\n",
    "            temp_KL[i] = 0.5 * (np.trace( np.dot(var_inv,self.Quu_inv_save[i,:,:])) \\\n",
    "                            + np.dot( np.dot( np.transpose(u_diff) ,var_inv), u_diff) \\\n",
    "                            - self.model.iu \\\n",
    "                            + np.log( np.linalg.det(var) / np.linalg.det(self.Quu_inv_save[i,:,:])) )\n",
    "        return np.sum( temp_KL )\n",
    "        \n",
    "    def estimate_cost(self,x,u,eta):\n",
    "        \n",
    "        ndim = np.ndim(x)\n",
    "        if ndim == 1: # 1 step state & input\n",
    "            N = 1\n",
    "            x = np.expand_dims(x,axis=0)\n",
    "            u = np.expand_dims(u,axis=0)\n",
    "        else :\n",
    "            N = np.size(x,axis = 0)\n",
    "                \n",
    "        # cost function for iLQR / dual variable\n",
    "        c1 = self.cost.estimateCost(x,u) / eta\n",
    "        \n",
    "        # policy PDF term\n",
    "        mean, var = self.policy.getPolicy(x)\n",
    "        var_inv = np.tile(np.linalg.inv(var),(N,1,1))\n",
    "        \n",
    "        pol_diff = np.expand_dims(u - mean,2)\n",
    "        c2 = 0.5 * np.matmul(np.matmul(np.transpose(pol_diff,(0,2,1)),var_inv),pol_diff)\n",
    "        \n",
    "        # divergence\n",
    "        return np.squeeze(c1 + c2);\n",
    "    \n",
    "    def diff_cost(self,x,u,eta) :\n",
    "        \n",
    "        # state & input size\n",
    "        ix = self.ix\n",
    "        iu = self.iu\n",
    "        \n",
    "        ndim = np.ndim(x)\n",
    "        if ndim == 1: # 1 step state & input\n",
    "            N = 1\n",
    "            x = np.expand_dims(x,axis=0)\n",
    "            u = np.expand_dims(u,axis=0)\n",
    "        else :\n",
    "            N = np.size(x,axis = 0)\n",
    "            \n",
    "        # cost function for modified cost\n",
    "        c1 = self.cost.diffCost(x,u) / eta\n",
    "        \n",
    "        # policy PDF term\n",
    "        mean, var = self.policy.getPolicy(x)\n",
    "        var_inv = np.tile(np.linalg.inv(var),(N,1,1))\n",
    "        \n",
    "        pol_diff = np.expand_dims(u - mean,2)\n",
    "        pol_jacobi = self.policy.jacobian(x)\n",
    "        \n",
    "        cx = np.matmul(np.matmul(np.transpose(pol_diff,(0,2,1)),var_inv), - pol_jacobi)\n",
    "        cu = np.matmul(np.transpose(pol_diff,(0,2,1)),var_inv)\n",
    "        \n",
    "        c2 = np.squeeze( np.dstack((cx,cu)) )\n",
    "\n",
    "        return  c1 + c2\n",
    "    \n",
    "    def hess_cost(self,x,u,eta):\n",
    "        \n",
    "        # state & input size\n",
    "        ix = self.ix\n",
    "        iu = self.iu\n",
    "        \n",
    "        ndim = np.ndim(x)\n",
    "        if ndim == 1: # 1 step state & input\n",
    "            N = 1\n",
    "            x = np.expand_dims(x,axis=0)\n",
    "            u = np.expand_dims(u,axis=0)\n",
    "        else :\n",
    "            N = np.size(x,axis = 0)\n",
    "        \n",
    "        # cost function for modified cost\n",
    "        c1 = self.cost.hessCost(x,u) / eta\n",
    "        \n",
    "        # policy PDF term\n",
    "        mean, var = self.policy.getPolicy(x)\n",
    "        var_inv = np.tile(np.linalg.inv(var),(N,1,1))\n",
    "        \n",
    "        pol_diff = np.expand_dims(u - mean,2)\n",
    "        pol_jacobi = self.policy.jacobian(x)\n",
    "        \n",
    "        cxx = np.matmul(np.matmul(np.transpose(-pol_jacobi,(0,2,1)),var_inv), - pol_jacobi)\n",
    "        cux = np.matmul( var_inv, -pol_jacobi)\n",
    "        cuu = var_inv\n",
    "        \n",
    "        c2 = np.squeeze(np.hstack((np.dstack((cxx,np.transpose(cux,(0,2,1)))),np.dstack((cux,cuu)))))\n",
    "         \n",
    "        return c1 + c2\n",
    "        \n",
    "    def forward(self,x0,u,K,x,k,alpha,eta,x0cov,fx,fu,Quu_inv):\n",
    "        # size\n",
    "        ix = self.model.ix\n",
    "        iu = self.model.iu\n",
    "        \n",
    "        # horizon\n",
    "        N = self.N\n",
    "        \n",
    "        # x-difference\n",
    "        dx = np.zeros(3)\n",
    "        \n",
    "        # jacobian\n",
    "#         print_np(fx)\n",
    "#         print_np(fu)\n",
    "        f = np.dstack((fx,fu))\n",
    "        \n",
    "        # variable setting\n",
    "        xnew = np.zeros((N+1,self.model.ix))\n",
    "        unew = np.zeros((N,self.model.iu))\n",
    "        cnew = np.zeros(N+1)\n",
    "        Snew = np.tile(0.01*np.identity(self.model.ix),[self.N+1,1,1])\n",
    "        Cnew = np.tile(0.01*np.identity(self.model.ix + self.model.iu),[self.N+1,1,1]) \n",
    "        \n",
    "        # initial state\n",
    "        xnew[0,:] = x0\n",
    "        Snew[0,:,:] = x0cov\n",
    "        Cnew[0,0:ix,0:ix] = Snew[0,:,:]\n",
    "        Cnew[0,0:ix,ix:ix+iu] = np.dot( Snew[0,:,:], K[0,:,:].T )\n",
    "        Cnew[0,ix:ix+iu,0:ix] = np.dot( K[0,:,:], Snew[0,:,:] )\n",
    "        Cnew[0,ix:ix+iu,ix:ix+iu] = np.dot( np.dot(K[0,:,:], Snew[0,:,:]), K[0,:,:].T ) + Quu_inv[0,:,:]\n",
    "        Snew[1,:,:] = np.dot(np.dot(f[0,:,:],Cnew[0,:,:]),f[0,:,:].T)\n",
    "        \n",
    "        # roll-out\n",
    "        for i in range(N):\n",
    "            dx = xnew[i,:] - x[i,:]\n",
    "            unew[i,:] = u[i,:] + k[i,:] * alpha + np.dot(K[i,:,:],dx)\n",
    "            xnew[i+1,:] = self.model.forwardDyn(xnew[i,:],unew[i,:])\n",
    "            cnew[i] = self.estimate_cost(xnew[i,:],unew[i,:],eta)\n",
    "            Cnew[i,0:ix,0:ix] = Snew[i,:,:]\n",
    "            Cnew[i,0:ix,ix:ix+iu] = np.dot( Snew[i,:,:], K[i,:,:].T )\n",
    "            Cnew[i,ix:ix+iu,0:ix] = np.dot( K[i,:,:], Snew[i,:,:] )\n",
    "            Cnew[i,ix:ix+iu,ix:ix+iu] = np.dot( np.dot(K[i,:,:], Snew[i,:,:]), K[i,:,:].T ) + Quu_inv[i,:,:]\n",
    "            Snew[i+1,:,:] = np.dot(np.dot(f[i,:,:],Cnew[i,:,:]),f[i,:,:].T)\n",
    "        \n",
    "        cnew[N] = self.estimate_cost(xnew[N,:],np.zeros(self.model.iu),eta)\n",
    "\n",
    "        return xnew,unew,cnew,Snew,Cnew\n",
    "        \n",
    "    def backward(self):\n",
    "        diverge = False\n",
    "        \n",
    "        # state & input size\n",
    "        ix = self.model.ix\n",
    "        iu = self.model.iu\n",
    "        \n",
    "        # V final value\n",
    "        self.Vx[self.N,:] = self.cx[self.N,:]\n",
    "        self.Vxx[self.N,:,:] = self.cxx[self.N,:,:]\n",
    "        \n",
    "        # Q function\n",
    "        Qu = np.zeros(iu)\n",
    "        Qx = np.zeros(ix)\n",
    "        Qux = np.zeros([iu,ix])\n",
    "        Quu = np.zeros([iu,iu])\n",
    "        Quu_save = np.zeros([self.N,iu,iu]) # for saving\n",
    "        Quu_inv_save = np.zeros([self.N,iu,iu])\n",
    "        Qxx = np.zeros([ix,ix])\n",
    "        \n",
    "        Vxx_reg = np.zeros([ix,ix])\n",
    "        Qux_reg = np.zeros([ix,iu])\n",
    "        QuuF = np.zeros([iu,iu])\n",
    "        \n",
    "        # open-loop gain, feedback gain\n",
    "        k_i = np.zeros(iu)\n",
    "        K_i = np.zeros([iu,ix])\n",
    "        \n",
    "        self.dV[0,0] = 0.0\n",
    "        self.dV[0,1] = 0.0\n",
    "        \n",
    "        diverge_test = 0\n",
    "        for i in range(self.N-1,-1,-1):\n",
    "            # print(i)\n",
    "            Qu = self.cu[i,:] + np.dot(self.fu[i,:].T, self.Vx[i+1,:])\n",
    "            Qx = self.cx[i,:] + np.dot(self.fx[i,:].T, self.Vx[i+1,:])\n",
    " \n",
    "            Qux = self.cxu[i,:,:].T + np.dot( np.dot(self.fu[i,:,:].T, self.Vxx[i+1,:,:]),self.fx[i,:,:])\n",
    "            Quu = self.cuu[i,:,:] + np.dot( np.dot(self.fu[i,:,:].T, self.Vxx[i+1,:,:]),self.fu[i,:,:])\n",
    "            Qxx = self.cxx[i,:,:] + np.dot( np.dot(self.fx[i,:,:].T, self.Vxx[i+1,:,:]),self.fx[i,:,:])\n",
    "            \n",
    "            Vxx_reg = self.Vxx[i+1,:,:] + self.lamda * np.identity(ix)\n",
    "            Qux_reg = self.cxu[i,:,:].T + np.dot(np.dot(self.fu[i,:,:].T, Vxx_reg), self.fx[i,:,:])\n",
    "            QuuF = self.cuu[i,:,:] + np.dot(np.dot(self.fu[i,:,:].T, Vxx_reg), self.fu[i,:,:])\n",
    "            Quu_save[i,:,:] = Quu\n",
    "            # add input constraints here!!\n",
    "        \n",
    "            \n",
    "            # control gain      \n",
    "            try:\n",
    "                R = sp.linalg.cholesky(QuuF,lower=False)\n",
    "            except sp.linalg.LinAlgError as err:\n",
    "                diverge_test = i+1\n",
    "                return diverge_test, Quu_save, Quu_inv_save\n",
    "                        \n",
    "            R_inv = sp.linalg.inv(R)\n",
    "            QuuF_inv = np.dot(R_inv,np.transpose(R_inv))\n",
    "            Quu_inv_save[i,:,:] = np.linalg.inv(Quu)\n",
    "            # Quu_inv_save[i,:,:] = QuuF_inv\n",
    "            k_i = - np.dot(QuuF_inv, Qu)\n",
    "            K_i = - np.dot(QuuF_inv, Qux_reg)\n",
    "            \n",
    "            # update cost-to-go approximation\n",
    "            self.dV[0,0] = np.dot(k_i.T, Qu) + self.dV[0,0]\n",
    "            self.dV[0,1] = 0.5*np.dot( np.dot(k_i.T, Quu), k_i) + self.dV[0,1]\n",
    "            self.Vx[i,:] = Qx + np.dot(np.dot(K_i.T,Quu),k_i) + np.dot(K_i.T,Qu) + np.dot(Qux.T,k_i)\n",
    "            self.Vxx[i,:,:] = Qxx + np.dot(np.dot(K_i.T,Quu),K_i) + np.dot(K_i.T,Qux) + np.dot(Qux.T,K_i)\n",
    "            self.Vxx[i,:,:] = 0.5 * ( self.Vxx[i,:,:].T + self.Vxx[i,:,:] )\n",
    "                                                                                               \n",
    "            # save the control gains\n",
    "            self.l[i,:] = k_i\n",
    "            self.L[i,:,:] = K_i\n",
    "            \n",
    "        return diverge_test, Quu_save, Quu_inv_save\n",
    "                   \n",
    "        \n",
    "    def update(self,x0,u0):\n",
    "        # current position\n",
    "        self.x0 = x0;\n",
    "        \n",
    "        # initial input\n",
    "        self.u = u0;\n",
    "        \n",
    "        # state & input & horizon size\n",
    "        ix = self.model.ix\n",
    "        iu = self.model.iu\n",
    "        N = self.N\n",
    "        \n",
    "        # initialzie parameters\n",
    "        self.lamda = 1.0;\n",
    "        self.dlamda = 1.0;\n",
    "        self.lamdaFactor = 1.6\n",
    "        \n",
    "        # boolian parameter setting\n",
    "        diverge = False\n",
    "        stop = False\n",
    "        \n",
    "        # trace for iteration\n",
    "        # timer, counters, constraints\n",
    "        # timer begin!!\n",
    "        \n",
    "        # generate initial trajectory\n",
    "        self.x[0,:] = self.x0\n",
    "        for j in range(np.size(self.Alpha,axis=0)):\n",
    "            for i in range(self.N):\n",
    "                self.x[i+1,:] = self.model.forwardDyn(self.x[i,:],self.Alpha[j]*self.u[i,:])  \n",
    "                self.c[i] = self.estimate_cost(self.x[i,:],self.Alpha[j]*self.u[i,:],self.eta)\n",
    "                if  np.max( self.x[i+1,:] ) > 1e8 :                \n",
    "                    diverge = True\n",
    "                    pass\n",
    "            self.c[self.N] = self.estimate_cost(self.x[self.N,:],np.zeros(self.model.iu),self.eta)\n",
    "            # self.c[self.N] = 0\n",
    "            if diverge == False:\n",
    "                break;\n",
    "                pass\n",
    "            pass\n",
    "        \n",
    "        # iterations starts!!\n",
    "        iteration = 0\n",
    "        flgChange = True\n",
    "        for iteration in range(self.maxIter) :\n",
    "            # differentiate dynamics and cost\n",
    "            # TODO - we need a more fast algorithm to calculate derivs of dyn, cost\n",
    "            #        1. using not for loop, but elementwise calculation\n",
    "            if flgChange == True:\n",
    "                start = time.clock()\n",
    "                self.fx, self.fu = self.model.diffDyn(self.x[0:N,:],self.u)\n",
    "                c_x_u = self.diff_cost(self.x[0:N,:],self.u,self.eta)\n",
    "                c_xx_uu = self.hess_cost(self.x[0:N,:],self.u,self.eta)\n",
    "                c_xx_uu = 0.5 * ( np.transpose(c_xx_uu,(0,2,1)) + c_xx_uu )\n",
    "                self.cx[0:N,:] = c_x_u[:,0:self.model.ix]\n",
    "                self.cu[0:N,:] = c_x_u[:,self.model.ix:self.model.ix+self.model.iu]\n",
    "                self.cxx[0:N,:,:] = c_xx_uu[:,0:ix,0:ix]\n",
    "                self.cxu[0:N,:,:] = c_xx_uu[:,0:ix,ix:(ix+iu)]\n",
    "                self.cuu[0:N,:,:] = c_xx_uu[:,ix:(ix+iu),ix:(ix+iu)]\n",
    "                c_x_u = self.diff_cost(self.x[N,:],np.zeros(iu),self.eta)\n",
    "                c_xx_uu = self.hess_cost(self.x[N,:],np.zeros(iu),self.eta)\n",
    "                # c_x_u = self.diff_cost(np.zeros(ix),np.zeros(iu))\n",
    "                # c_xx_uu = self.hess_cost(np.zeros(ix),np.zeros(iu))\n",
    "                c_xx_uu = 0.5 * ( c_xx_uu + c_xx_uu.T)\n",
    "                self.cx[N,:] = c_x_u[0:self.model.ix]\n",
    "                self.cxx[N,:,:] = c_xx_uu[0:ix,0:ix]\n",
    "                flgChange = False\n",
    "                pass\n",
    "            time_derivs = (time.clock() - start)\n",
    "            \n",
    "            # backward pass\n",
    "            backPassDone = False;\n",
    "            while backPassDone == False:\n",
    "                start =time.clock()\n",
    "                diverge,self.Quu_save,self.Quu_inv_save = self.backward()\n",
    "                time_backward = time.clock() - start\n",
    "                if diverge != 0 :\n",
    "                    if self.verbosity == True:\n",
    "                        print(\"Cholesky failed at %s\" % (diverge))\n",
    "                        pass\n",
    "                    self.dlamda = np.maximum(self.dlamda * self.lamdaFactor, self.lamdaFactor)\n",
    "                    self.lamda = np.maximum(self.lamda * self.dlamda,self.lamdaMin)\n",
    "                    if self.lamda > self.lamdaMax :\n",
    "                        break\n",
    "                        pass\n",
    "                    continue\n",
    "                    pass\n",
    "                backPassDone = True\n",
    "                \n",
    "            # check for termination due to small gradient\n",
    "            g_norm = np.mean( np.max( np.abs(self.l) / (np.abs(self.u) + 1), axis=1 ) )\n",
    "            if g_norm < self.tolGrad and self.lamda < 1e-5 :\n",
    "                self.dlamda = np.minimum(self.dlamda / self.lamdaFactor, 1/self.lamdaFactor)\n",
    "                if self.lamda > self.lamdaMin :\n",
    "                    temp_c = 1\n",
    "                    pass\n",
    "                else :\n",
    "                    temp_c = 0\n",
    "                    pass       \n",
    "                self.lamda = self.lamda * self.dlamda * temp_c \n",
    "                if self.verbosity == True:\n",
    "                    print(\"SUCCEESS : gradient norm < tolGrad\")\n",
    "                    pass\n",
    "                break\n",
    "                pass\n",
    "            \n",
    "\n",
    "            # step3. line-search to find new control sequence, trajectory, cost\n",
    "            fwdPassDone = False\n",
    "            if backPassDone == True :\n",
    "                start = time.clock()\n",
    "                for i in self.Alpha :\n",
    "                    self.xnew,self.unew,self.cnew,self.Snew,self.Cnew = self.forward(self.x0,self.u,self.L,self.x,\\\n",
    "                                                                 self.l,i,self.eta, \\\n",
    "                                                                 self.x0cov,self.fx,self.fu,self.Quu_inv_save)\n",
    "                    dcost = np.sum( self.c ) - np.sum( self.cnew )\n",
    "                    expected = -i * (self.dV[0,0] + i * self.dV[0,1])\n",
    "                    if expected > 0 :\n",
    "                        z = dcost / expected\n",
    "                    else :\n",
    "                        z = np.sign(dcost)\n",
    "                        print(\"non-positive expected reduction: should not occur\")\n",
    "                        pass\n",
    "                    # print(i)\n",
    "                    if z > self.zMin :\n",
    "                        fwdPassDone = True\n",
    "                        break          \n",
    "                if fwdPassDone == False :\n",
    "                    alpha_temp = 1e8 # % signals failure of forward pass\n",
    "                    pass\n",
    "                time_forward = time.clock() - start\n",
    "            # step4. accept step, draw graphics, print status \n",
    "            if self.verbosity == True and self.last_head == True:\n",
    "                self.last_head = False\n",
    "                print(\"iteration   cost        reduction   expected    gradient    log10(lambda)\")\n",
    "                pass\n",
    "\n",
    "            if fwdPassDone == True:\n",
    "                if self.verbosity == True:\n",
    "                    print(\"%-12d%-12.6g%-12.3g%-12.3g%-12.3g%-12.1f\" % ( iteration,np.sum(self.c),dcost,expected,g_norm,np.log10(self.lamda)) )     \n",
    "                    pass\n",
    "\n",
    "                # decrese lamda\n",
    "                self.dlamda = np.minimum(self.dlamda / self.lamdaFactor, 1/self.lamdaFactor)\n",
    "                if self.lamda > self.lamdaMin :\n",
    "                    temp_c = 1\n",
    "                    pass\n",
    "                else :\n",
    "                    temp_c = 0\n",
    "                    pass\n",
    "                self.lamda = self.lamda * self.dlamda * temp_c \n",
    "\n",
    "                # accept changes\n",
    "                self.u = self.unew\n",
    "                self.x = self.xnew\n",
    "                self.c = self.cnew\n",
    "                self.S = self.Snew\n",
    "                self.C = self.Cnew\n",
    "                flgChange = True\n",
    "#                 print(time_derivs)\n",
    "#                 print(time_backward)\n",
    "#                 print(time_forward)\n",
    "                # abc\n",
    "                # terminate?\n",
    "                if dcost < self.tolFun :\n",
    "                    if self.verbosity == True:\n",
    "                        print(\"SUCCEESS: cost change < tolFun\")\n",
    "                        pass\n",
    "                    break\n",
    "                    pass\n",
    "                \n",
    "            else : # no cost improvement\n",
    "                # increase lamda\n",
    "                # ssprint(iteration)\n",
    "                self.dlamda = np.maximum(self.dlamda * self.lamdaFactor, self.lamdaFactor)\n",
    "                self.lamda = np.maximum(self.lamda * self.dlamda,self.lamdaMin)\n",
    "\n",
    "                # print status\n",
    "                if self.verbosity == True :\n",
    "                    print(\"%-12d%-12s%-12.3g%-12.3g%-12.3g%-12.1f\" %\n",
    "                     ( iteration,'NO STEP', dcost, expected, g_norm, np.log10(self.lamda) ));\n",
    "                    pass\n",
    "\n",
    "                if self.lamda > self.lamdaMax :\n",
    "                    if self.verbosity == True:\n",
    "                        print(\"EXIT : lamda > lamdaMax\")\n",
    "                        pass\n",
    "                    break\n",
    "                    pass\n",
    "                pass\n",
    "            pass\n",
    "        return self.x, self.u, self.Quu_save, self.Quu_inv_save\n",
    "    \n",
    "    def iterDGD(self,x0,uIni) :\n",
    "        \n",
    "        eta_max = 1.5\n",
    "        eta_min = 0.1\n",
    "        \n",
    "        maxIterDGD = 15\n",
    "        print(\"DGD starts!! eta = \", self.eta)\n",
    "        for i in range(maxIterDGD) :\n",
    "            x_temp, u_temp, Quu_temp, Quu_inv_temp = self.update(x0,uIni)\n",
    "            cost = self.getCost(x_temp,u_temp)\n",
    "            kl = self.getKL(x_temp,u_temp)\n",
    "            print(\"cost = \", cost, \"KL = \", kl, \"epsilon = \", self.epsilon)\n",
    "            if kl <= self.epsilon :\n",
    "                if kl > 0.9 * self.epsilon :\n",
    "                    print(\"dual gradient descent is converged\")\n",
    "                    print(\"eta = \", self.eta)\n",
    "                    break\n",
    "                else :\n",
    "                    print(\"KL < epsilon\")\n",
    "                    eta_max = self.eta\n",
    "                    geom = np.sqrt(eta_max * eta_min)\n",
    "                    self.eta = np.maximum(0.1*eta_max,geom)\n",
    "                    print(\"eta = \", self.eta)\n",
    "            else :\n",
    "                print(\"KL > epsilon\")\n",
    "                eta_min = self.eta\n",
    "                geom = np.sqrt(eta_max * eta_min)\n",
    "                self.eta = np.minimum(10 * eta_min,geom)\n",
    "                print(\"eta = \", self.eta)\n",
    "                \n",
    "        return x_temp, u_temp, Quu_temp, Quu_inv_temp, self.eta\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   cost        reduction   expected    gradient    log10(lambda)\n",
      "0           1051.97     810         464         1.4         0.0         \n",
      "1           241.632     144         146         0.741       -0.2        \n",
      "2           97.4551     5.34        7.74        0.154       -0.6        \n",
      "3           92.115      0.343       2.18        0.0336      -1.2        \n",
      "4           91.7716     0.155       1.61        0.0189      -2.0        \n",
      "5           91.6168     0.14        1.34        0.0181      -3.1        \n",
      "6           91.4768     0.0957      1.07        0.0145      -4.3        \n",
      "7           91.3811     0.0897      0.898       0.0142      -5.7        \n",
      "8           91.2914     0.0638      0.723       0.0119      -7.3        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iLQR.py:306: RuntimeWarning: divide by zero encountered in log10\n",
      "  print(\"%-12d%-12.6g%-12.3g%-12.3g%-12.3g%-12.1f\" % ( iteration,np.sum(self.c),dcost,expected,g_norm,np.log10(self.lamda)) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9           91.2276     0.0591      0.607       0.0115      -inf        \n",
      "10          91.1685     0.0432      0.492       0.00983     -inf        \n",
      "11          91.1253     0.0394      0.413       0.00945     -inf        \n",
      "12          91.0859     0.0294      0.336       0.00816     -inf        \n",
      "13          91.0565     0.0265      0.282       0.00776     -inf        \n",
      "14          91.03       0.0202      0.23        0.00677     -inf        \n",
      "10.6331\n",
      "0.0269596\n",
      "0.0171466\n",
      "0.0127848\n",
      "0.0100004\n",
      "0.00799054\n",
      "0.00646466\n",
      "0.00527853\n",
      "0.00434435\n",
      "0.00360231\n",
      "Optimization - policy mean is done\n",
      "Optimization - policy variance is done\n"
     ]
    }
   ],
   "source": [
    "# # Initial trajectory distribution from iLQR with low number of iterations\n",
    "i1 = iLQR.iLQR('unicycle',400,15)\n",
    "x0 = np.zeros(3)\n",
    "x0[0] = 5\n",
    "x0[1] = 5\n",
    "x0[2] = np.pi / 2\n",
    "u0 = np.zeros( (400,2) )\n",
    "x,u, Quu, Quu_inv = i1.update(x0,u0)\n",
    "# policy learining\n",
    "xData = x[0:400,:]\n",
    "myPolicy = poliOpt.poliOpt('test',10,5000,0.01,3,2)\n",
    "myPolicy.setInitial(5000,0.01)\n",
    "W1,b1,W2,b2,var = myPolicy.update(xData,u,Quu)\n",
    "np.savetxt('w1.txt',W1)\n",
    "np.savetxt('w2.txt',W2)\n",
    "np.savetxt('b1.txt',b1)\n",
    "np.savetxt('b2.txt',b2)\n",
    "np.savetxt('var.txt',var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trajectory optimization\n",
    "myTraj = trajOpt('Hi',400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myTraj.setEnv(myPolicy,1,100)\n",
    "x_new, u_new, Quu_new, Quu_inv_new, eta_new = myTraj.iterDGD(x0,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xData = x_new[0:400,:]\n",
    "myPolicy = poliOpt.poliOpt('test',10,5000,0.01,3,2)\n",
    "W1,b1,W2,b2,var = myPolicy.update(xData,u_new,Quu_new)\n",
    "np.savetxt('w1.txt',W1)\n",
    "np.savetxt('w2.txt',W2)\n",
    "np.savetxt('b1.txt',b1)\n",
    "np.savetxt('b2.txt',b2)\n",
    "np.savetxt('var.txt',var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
