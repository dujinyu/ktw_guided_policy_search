{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.linalg\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "def print_np(x):\n",
    "    print (\"Type is %s\" % (type(x)))\n",
    "    print (\"Shape is %s\" % (x.shape,))\n",
    "    print (\"Values are: \\n%s\" % (x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import model\n",
    "import cost\n",
    "import iLQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   cost        reduction   expected    gradient    log10(lambda)\n",
      "0           1051.97     810         464         1.4         0.0         \n",
      "1           241.632     144         146         0.741       -0.2        \n",
      "2           97.4551     5.34        7.74        0.154       -0.6        \n",
      "3           92.115      0.343       2.18        0.0336      -1.2        \n",
      "4           91.7716     0.155       1.61        0.0189      -2.0        \n",
      "5           91.6168     0.14        1.34        0.0181      -3.1        \n",
      "6           91.4768     0.0957      1.07        0.0145      -4.3        \n",
      "7           91.3811     0.0897      0.898       0.0142      -5.7        \n",
      "8           91.2914     0.0638      0.723       0.0119      -7.3        \n",
      "9           91.2276     0.0591      0.607       0.0115      -inf        \n",
      "10          91.1685     0.0432      0.492       0.00983     -inf        \n",
      "11          91.1253     0.0394      0.413       0.00945     -inf        \n",
      "12          91.0859     0.0294      0.336       0.00816     -inf        \n",
      "13          91.0565     0.0265      0.282       0.00776     -inf        \n",
      "14          91.03       0.0202      0.23        0.00677     -inf        \n",
      "15          91.0098     0.0179      0.193       0.00639     -inf        \n",
      "16          90.9919     0.0138      0.158       0.00562     -inf        \n",
      "17          90.9781     0.0122      0.132       0.00527     -inf        \n",
      "18          90.9659     0.00952     0.108       0.00467     -inf        \n",
      "19          90.9564     0.00829     0.0904      0.00435     -inf        \n",
      "20          90.9481     0.00655     0.0744      0.00387     -inf        \n",
      "21          90.9415     0.00566     0.062       0.0036      -inf        \n",
      "22          90.9359     0.00451     0.0511      0.00322     -inf        \n",
      "23          90.9314     0.00387     0.0426      0.00297     -inf        \n",
      "24          90.9275     0.00311     0.0351      0.00267     -inf        \n",
      "25          90.9244     0.00265     0.0293      0.00246     -inf        \n",
      "26          90.9217     0.00214     0.0242      0.00221     -inf        \n",
      "27          90.9196     0.00182     0.0201      0.00204     -inf        \n",
      "28          90.9178     0.00147     0.0166      0.00184     -inf        \n",
      "29          90.9163     0.00125     0.0138      0.00169     -inf        \n",
      "30          90.915      0.00101     0.0114      0.00152     -inf        \n",
      "31          90.914      0.000856    0.0095      0.0014      -inf        \n",
      "32          90.9132     0.000698    0.00786     0.00127     -inf        \n",
      "33          90.9125     0.000587    0.00653     0.00116     -inf        \n",
      "34          90.9119     0.00048     0.0054      0.00105     -inf        \n",
      "35          90.9114     0.000403    0.00449     0.000961    -inf        \n",
      "36          90.911      0.00033     0.00372     0.000871    -inf        \n",
      "37          90.9107     0.000277    0.00309     0.000796    -inf        \n",
      "38          90.9104     0.000227    0.00255     0.000722    -inf        \n",
      "39          90.9102     0.00019     0.00212     0.00066     -inf        \n",
      "40          90.91       0.000156    0.00176     0.000599    -inf        \n",
      "41          90.9098     0.000131    0.00146     0.000547    -inf        \n",
      "42          90.9097     0.000108    0.00121     0.000497    -inf        \n",
      "43          90.9096     9e-05       0.001       0.000454    -inf        \n",
      "44          90.9095     7.39e-05    0.000831    0.000412    -inf        \n",
      "45          90.9094     6.19e-05    0.000689    0.000376    -inf        \n",
      "46          90.9094     5.08e-05    0.000571    0.000342    -inf        \n",
      "47          90.9093     4.26e-05    0.000474    0.000312    -inf        \n",
      "48          90.9093     3.49e-05    0.000393    0.000284    -inf        \n",
      "49          90.9092     2.93e-05    0.000326    0.000259    -inf        \n",
      "50          90.9092     2.4e-05     0.00027     0.000235    -inf        \n",
      "51          90.9092     2.01e-05    0.000224    0.000214    -inf        \n",
      "52          90.9092     1.65e-05    0.000186    0.000195    -inf        \n",
      "53          90.9091     1.39e-05    0.000154    0.000178    -inf        \n",
      "54          90.9091     1.13e-05    0.000128    0.000162    -inf        \n",
      "55          90.9091     9.54e-06    0.000106    0.000147    -inf        \n",
      "56          90.9091     7.79e-06    8.79e-05    0.000134    -inf        \n",
      "57          90.9091     6.57e-06    7.29e-05    0.000122    -inf        \n",
      "58          90.9091     5.35e-06    6.04e-05    0.000111    -inf        \n",
      "59          90.9091     4.53e-06    5.01e-05    0.000101    -inf        \n",
      "60          90.9091     3.67e-06    4.15e-05    9.22e-05    -inf        \n",
      "61          90.9091     3.12e-06    3.45e-05    8.4e-05     -inf        \n",
      "62          90.9091     2.52e-06    2.86e-05    7.65e-05    -inf        \n",
      "63          90.9091     2.15e-06    2.37e-05    6.97e-05    -inf        \n",
      "64          90.9091     1.73e-06    1.96e-05    6.34e-05    -inf        \n",
      "65          90.9091     1.48e-06    1.63e-05    5.78e-05    -inf        \n",
      "66          90.9091     1.18e-06    1.35e-05    5.26e-05    -inf        \n",
      "67          90.9091     1.02e-06    1.12e-05    4.79e-05    -inf        \n",
      "68          90.9091     8.11e-07    9.29e-06    4.36e-05    -inf        \n",
      "SUCCEESS: cost change < tolFun\n"
     ]
    }
   ],
   "source": [
    "# # Initial trajectory distribution from iLQR with low number of iterations\n",
    "# i1 = iLQR.iLQR('unicycle',400,100)\n",
    "# x0 = np.zeros(3)\n",
    "# x0[0] = 5\n",
    "# x0[1] = 5\n",
    "# x0[2] = np.pi / 2\n",
    "# u0 = np.zeros( (400,2) )\n",
    "# x,u, Quu, Quu_inv = i1.update(x0,u0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class poliOpt:\n",
    "    def __init__(self,name,x,u,Q,hidden_num,maxIter,stepSize):\n",
    "        # name\n",
    "        self.name = name\n",
    "        self.maxIter = maxIter\n",
    "        self.stepSize = stepSize\n",
    "        self.hidden_num = hidden_num\n",
    "        \n",
    "        # data\n",
    "        self.state_label = x\n",
    "        self.input_label = u\n",
    "        self.var_inv_label = Q\n",
    "        \n",
    "        # size\n",
    "        self.ix = x.shape[1]\n",
    "        self.iu = u.shape[1]\n",
    "        \n",
    "        # policy variance <- constant when optimization was done\n",
    "        self.policy_var = np.zeros((self.iu,self.iu))\n",
    "        \n",
    "        # Initialize tensorflow setting\n",
    "        self.setInitial(self.stepSize,self.maxIter)\n",
    "\n",
    "\n",
    "    def setInitial(self,stepSize,maxIter) :\n",
    "        \n",
    "        self.stepSize = stepSize\n",
    "        self.maxIter = maxIter\n",
    "        \n",
    "        # tensorflow variables & parameters\n",
    "        self.Weight = tf.placeholder(tf.float32, [None,self.iu,self.iu])\n",
    "        self.input_x = tf.placeholder(tf.float32, [None,self.ix])\n",
    "        self.input_y = tf.placeholder(tf.float32, [None,self.iu])\n",
    "        \n",
    "        # variables\n",
    "        self.W1 = tf.Variable(tf.random_normal(shape = [self.ix,self.hidden_num]),dtype = tf.float32)\n",
    "        self.b1 = tf.Variable(tf.random_normal(shape = [self.hidden_num]),dtype =  tf.float32)\n",
    "\n",
    "        self.W2 = tf.Variable(tf.random_normal(shape = [self.hidden_num,self.iu]),dtype =  tf.float32)\n",
    "        self.b2 = tf.Variable(tf.random_normal(shape = [self.iu]),dtype = tf.float32)\n",
    "\n",
    "        self.layer1 = tf.nn.softplus(tf.add(tf.matmul(self.input_x,self.W1),self.b1))\n",
    "        self.layer2 = tf.add(tf.matmul(self.layer1, self.W2), self.b2)\n",
    "        self.Y_pred = self.layer2\n",
    "        \n",
    "         # loss function\n",
    "        self.u_error = tf.expand_dims(self.Y_pred - self.input_y,2)\n",
    "        self.loss = tf.reduce_mean(tf.matmul( tf.matmul(tf.transpose(self.u_error,perm=[0,2,1]), self.Weight\n",
    "                    ),self.u_error) )\n",
    "        \n",
    "        # self.optimizer = tf.train.AdamOptimizer(self.stepSize).minimize(self.loss)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.stepSize).minimize(self.loss)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init)   \n",
    "        \n",
    "    def setData(self,x,u,Q) :\n",
    "        \n",
    "        # change data\n",
    "        self.state_label = x\n",
    "        self.input_label = u\n",
    "        self.var_inv_label = Q\n",
    "        \n",
    "    def meanOpt(self) :\n",
    "        \n",
    "        for i in range(self.maxIter) :\n",
    "            self.sess.run(self.optimizer, feed_dict={self.input_x: self.state_label, \\\n",
    "                                                     self.input_y: self.input_label, \\\n",
    "                                                     self.Weight : self.var_inv_label})\n",
    "            if i % 500 == 0 :\n",
    "                loss_temp = self.sess.run(self.loss,feed_dict={self.input_x: self.state_label,\\\n",
    "                                                         self.input_y: self.input_label, \\\n",
    "                                                         self.Weight : self.var_inv_label})\n",
    "                print(loss_temp)\n",
    "                if loss_temp < 0.1 :\n",
    "                    break\n",
    "        print(\"Optimization - policy mean is done\")\n",
    "        \n",
    "    def varOpt(self) :\n",
    "        # data\n",
    "        Q = self.var_inv_label\n",
    "        \n",
    "        # optimization\n",
    "        cov_inv = Q.sum(axis=0) / np.size(Q,axis=0)\n",
    "        self.policy_var = np.linalg.inv(cov_inv)\n",
    "        \n",
    "        print(\"Optimization - policy variance is done\")\n",
    "\n",
    "    def getWeight(self) :\n",
    "        \n",
    "        return self.sess.run([self.W1, self.b1, self.W2, self.b2])\n",
    "        \n",
    "    def getPolicy(self,x) :\n",
    "        \n",
    "        # stochastic policy\n",
    "        \n",
    "        # deterministic policy\n",
    "        return self.sess.run(self.Y_pred,feed_dict={self.input_x: x}), self.policy_var\n",
    "    \n",
    "    def gaussPDF(self,x,u) :\n",
    "        \n",
    "        ndim = np.ndim(u)\n",
    "        if ndim == 1: # 1 step state & input\n",
    "            N = 1\n",
    "            x = np.expand_dims(x,axis=0)\n",
    "            u = np.expand_dims(u,axis=0)\n",
    "        else :\n",
    "            N = np.size(x,axis = 0)\n",
    "        \n",
    "        # mean & variance of policy\n",
    "        mean = self.sess.run(self.Y_pred,feed_dict={self.input_x: x})\n",
    "        var = self.policy_var\n",
    "        \n",
    "        # expand dims to tensor\n",
    "        u = np.expand_dims(u,axis=2)\n",
    "        mean = np.expand_dims(mean,axis=2)\n",
    "\n",
    "        error = u - mean\n",
    "        var_inv = np.linalg.inv(var)\n",
    "        \n",
    "        temp = np.matmul(np.matmul( np.transpose(error,(0,2,1)),var_inv),error)\n",
    "\n",
    "        p = 1 / ( np.sqrt( np.abs( 2 * np.pi * np.linalg.det(var) ))) \\\n",
    "                 * np.exp( - 0.5 * temp)\n",
    "            \n",
    "        return np.squeeze(p)\n",
    "            \n",
    "    def update(self,x,u,Q) :\n",
    "        \n",
    "        # data setting\n",
    "        self.setData(x,u,Q)\n",
    "        \n",
    "        # mean optimization - backpropagation\n",
    "        self.meanOpt()\n",
    "        \n",
    "        # variance optimization\n",
    "        self.varOpt()\n",
    "\n",
    "        # get parameters of network\n",
    "        W1,b1,W2,b2 = self.getWeight()\n",
    "        \n",
    "        # return parameter, variance of policy\n",
    "        return W1,b1,W2,b2, self.policy_var\n",
    "    \n",
    "    def jacobian(self,x) :\n",
    "        \n",
    "        ndim = np.ndim(x)\n",
    "        if ndim == 1: # 1 step state & input\n",
    "            N = 1\n",
    "        else :\n",
    "            N = np.size(x,axis = 0)\n",
    "            \n",
    "        # temp = tf.placeholder(tf.float32, [1,None,self.iu])\n",
    "        temp = [[] for i in range(self.iu)]\n",
    "        grad_nn = [[] for i in range(self.iu)]\n",
    "        out = [[] for i in range(self.iu)]\n",
    "        for i in range(self.iu) :\n",
    "            temp[i] = self.Y_pred[:,i]\n",
    "            grad_nn[i] = tf.gradients(temp[i],self.input_x)\n",
    "            out[i] = np.transpose( np.array( self.sess.run(grad_nn[i],feed_dict={self.input_x: x}) ) , (1,0,2))\n",
    "            \n",
    "        return np.hstack((out[i] for i in range(self.iu)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0595\n",
      "0.0779878\n",
      "Optimization - policy mean is done\n",
      "Optimization - policy variance is done\n"
     ]
    }
   ],
   "source": [
    "# # x,u, Quu_save, Quu_inv_save = i1.update(x0,u0)\n",
    "# xData = x[0:400,:]\n",
    "# myPolicy = poliOpt('test',xData,u,Quu,10,5000,0.01)\n",
    "# W1,b1,W2,b2,var = myPolicy.update(xData,u,Quu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
